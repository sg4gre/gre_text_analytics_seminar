{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c5e461a",
   "metadata": {},
   "source": [
    "# Introduction to text analytics seminar.\n",
    "\n",
    "Author: Dr Stef Garasto.\n",
    "\n",
    "Licence: [GPLv3](https://choosealicense.com/licenses/gpl-3.0/).\n",
    "\n",
    "\n",
    "Python script to reproduce (approximately and partially) the paper [\"Analyzing Gender Bias within Narrative Tropes\"](https://aclanthology.org/2020.nlpcss-1.23.pdf) by Gala et al. (2020), part 2.\n",
    "\n",
    "The goal is to show some useful text analytics techniques in context.\n",
    "\n",
    "Specifically, here we will see:\n",
    "\n",
    "- a bit about **approximate** matching of specific keywords in a text (using word embeddings).\n",
    "- topic modelling using Latent Dirichlet Allocation.\n",
    "\n",
    "(Some of the initial sections are the same as part 1 because we need to pre-process the data again)\n",
    "\n",
    "These techniques are applied to try and quantify the **binary** \"genderedness\" of tropes and topics within those tropes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbcd001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import cleantext\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5188c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e135240",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b9eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_aug_tropes= 'https://raw.githubusercontent.com/sg4gre/gre_ta_seminar_data/main/TVTropesData/partial_augmented_tropes.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e88c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tropes and example occurrences in films\n",
    "try:\n",
    "    augmented_tropes = pd.read_csv('../gre_ta_seminar_data/TvTropesData/partial_augmented_tropes.csv')\n",
    "except:\n",
    "    augmented_tropes = pd.read_csv(url_aug_tropes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09663b78",
   "metadata": {},
   "source": [
    "## A quick further look at the word matching\n",
    "\n",
    "Using Spacy's PhraseMatcher, we matched **exact** occurrences of the \"male\" and \"female\" words as they appear in the tropes' descriptions and examples. \n",
    "\n",
    "What if there are spelling mistakes? What if we want to also look for **similar** words, without needing to re-do the original list every time.\n",
    "\n",
    "In short, we might want to do some \"fuzzy\" (that is, non-exact) matching.\n",
    "\n",
    "Two possible tools that may help with this are: a) comparing words using the Levenshtein distance and b) comparing words using word embeddings. \n",
    "\n",
    "While we won't see the Levensthein distance in this tutorial, briefly this is a tool that allows us to quantify how similar two words are based on the 'minimum number of single-character edits required to change one word into the other'. For example, 'trope' and 'trpe' would have a distance of 1. This can be a good tool to catch spelling mistakes, though be careful: also 'take' and 'bake' have a distance of 1! A good python implementation is the [fuzzywuzzy](https://pypi.org/project/fuzzywuzzy/) package.\n",
    "\n",
    "Let's see what we can do with embeddings.\n",
    "\n",
    "#### Using embeddings for similarity matching\n",
    "\n",
    "As mentioned, we can represent words (and text data in general) via embeddings. A word embedding is a representation of a word via a list of numbers, where each vector is computed in a way that attempts to preserve the semantic meaning of each word. These vectors are called dense since most of their entries will not be zero. \n",
    "\n",
    "This allows to store information in a more compact manner and therefore reduce the dimensionality of our data. Word embeddings are usually trained on large collections of text, since they have to learn and leverage statistical patterns in text data. A famous word embedding model is [\"Word2Vec\"](https://arxiv.org/abs/1301.3781) by Mikolov et al.\n",
    "\n",
    "The good news is that spaCy comes with word embeddings that someone else has already trained! (Although the documentation for how exactly they are trained is sparse). \n",
    "That is, Spacy has a mapping from words to pre-made lists of numbers that are meant to encode the semantic meaning of those words (it's not **all** possible words, but it's typically a large enough vocabulary).\n",
    "\n",
    "With spaCy we can obtain a vector embedding for each of the tokens in the text. spaCy even provides a vector for the whole document which is given by the average vector embedding across all tokens in the document (this strategy often works better for short texts).\n",
    "\n",
    "We can access the word/document embeddings via the .vector attribute.\n",
    "\n",
    "Once we have embeddings, we can compute how similar two words are based on how similar their embeddings are. The similarity of two embeddings is computed using a metric called cosine similarity. This is a number between -1 and 1, where:\n",
    "\n",
    "- -1 suggests two words are \"antonyms\".\n",
    "- 0 suggests two words are completely unrelated.\n",
    "- 1 suggests two words are \"synonyms\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13bff22",
   "metadata": {},
   "source": [
    "##### Implementation note\n",
    "\n",
    "Lastly, a note about Spacy.\n",
    "\n",
    "When we call `nlp= spacy.load('en_core_web_md')` (or similar), what we are doing is importing a collection of pre-made algorithms into our python workspaces so that we can apply them to our dataset.\n",
    "\n",
    "`'en_core_web_sm'`, `'en_core_web_md'` and `'en_core_web_lg'` are three slightly different collections of NLP algorithms, also called NLP pipelines.\n",
    "\n",
    "'sm' (=small), 'md' (=medium) and 'lg' (=large) refer to the \"size\" of the algorithms: the bigger the size the more complex the algorithms are. Complexity means that they are more powerul, but also take up more memory and computational resources. The suggestion is to choose the minimal complexity needed to solve your specific problem.\n",
    "\n",
    "There is an important note to make when using word embeddings. `'en_core_web_sm'` is too small a Spacy pipeline **and does not come with word embeddings**. If you want to use embeddings, you need to choose either `'en_core_web_md'` or `'en_core_web_lg'`. I suggest starting from `'en_core_web_md'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd26db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the proper Spacy pipeline (before we use en_core_web_md which does not have a vector per token)\n",
    "nlp_md = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "t0= time.time()\n",
    "# For speed, we will only apply it to the first 20 tropes only\n",
    "docs_md = list(nlp_md.pipe(augmented_tropes.sort_values(by='GenderedTokensNb',ascending=False)['DescAndExamples'].head(20), \n",
    "            ##disabling NLP capabilities that are irrelevant for the task at hand can speed things up\n",
    "           disable=[\"ner\",\"parser\",\"textcat\"])) \n",
    "print(f'Overall it took {(time.time()-t0)/60:.3f} minutes.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a804ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's obtain the embedding for each gendered word\n",
    "\n",
    "# this is how many numbers are in each embedding (the dimension of the embedding)\n",
    "emb_dim = len(docs_md[0][0].vector)\n",
    "\n",
    "# \"male\" words\n",
    "docs_male = nlp_md.pipe(male_words)\n",
    "# this create a matrix (a collection of numbers stored as a table) that initially contains all 0s. \n",
    "# We create a table with nb of rows = nb of male words and nb of columns = the dimension of the embedding\n",
    "male_embs = np.zeros((len(male_words), emb_dim)) \n",
    "for i,doc in enumerate(docs_male):\n",
    "    male_embs[i,:] = doc.vector\n",
    "\n",
    "# same for \"female\" words\n",
    "docs_female = nlp_md.pipe(female_words)\n",
    "female_embs = np.zeros((len(female_words), emb_dim))\n",
    "for i,doc in enumerate(docs_female):\n",
    "    female_embs[i,:] = doc.vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f88a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# let's try it on the first trope\n",
    "# first, we will store an embedding for each token.\n",
    "doc = docs_md[1]\n",
    "doc_matrix_embs = np.zeros((len(doc), emb_dim))\n",
    "for i,token in enumerate(doc):\n",
    "    doc_matrix_embs[i,:] = token.vector\n",
    "    \n",
    "# now compute the cosine similarity between each token and each \"male\" word\n",
    "trope_male_similarity = cosine_similarity(doc_matrix_embs,male_embs)\n",
    "\n",
    "# We define a threshold: two words match if their cosine similarity is more than 0.99.\n",
    "similarity_threshold = .99\n",
    "# For each token in a trope, it is a match if it matches at least one of the \"male\" words with a cosine similarity > 0.99\n",
    "matched_words = []\n",
    "for i in range(trope_male_similarity.shape[0]):\n",
    "    if (trope_male_similarity[i,:]>similarity_threshold).any():\n",
    "        if doc[i].text not in matched_words:\n",
    "            print('Matched word:', doc[i].text, f'. Is it in the original list? {doc[i].text in male_words}')\n",
    "            matched_words.append(doc[i].text)\n",
    "# example new words matched --> strong, fireman (can be a bit cyclical), but also widow, mother (!).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c5aefd",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Try and modify the code below so that we explore the word embedding matches for a **different trope**.\n",
    "\n",
    "For example, instead of using the first trope, what happens if we check the fourth one?\n",
    "\n",
    "Then/Or, try to decrease the similarity threshold from .99 to .95. Do we get more useful or misleading words?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ce9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try it on the fourth trope this time\n",
    "# first, we will store an embedding for each token.\n",
    "doc = docs_md[1]\n",
    "doc_matrix_embs = np.zeros((len(doc), emb_dim))\n",
    "for i,token in enumerate(doc):\n",
    "    doc_matrix_embs[i,:] = token.vector\n",
    "    \n",
    "# now compute the cosine similarity between each token and each \"male\" word\n",
    "trope_male_similarity = cosine_similarity(doc_matrix_embs,male_embs)\n",
    "\n",
    "# We define a threshold: two words match if their cosine similarity is more than 0.99.\n",
    "similarity_threshold = .99\n",
    "# For each token in a trope, it is a match if it matches at least one of the \"male\" words with a cosine similarity > 0.99\n",
    "matched_words = []\n",
    "for i in range(trope_male_similarity.shape[0]):\n",
    "    if (trope_male_similarity[i,:]>similarity_threshold).any():\n",
    "        if doc[i].text not in matched_words:\n",
    "            print('Matched word:', doc[i].text, f'. Is it in the original list? {doc[i].text in male_words}')\n",
    "            matched_words.append(doc[i].text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ae189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76db1009",
   "metadata": {},
   "source": [
    "# Topic Modelling using Latent Dirichlet Allocation\n",
    "\n",
    "(Extracted and adapted from: https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py (Â© Copyright 2007 - 2024, scikit-learn developers (BSD License)). Code is provided \"as is\" (full disclaimer notice at the end). )\n",
    "\n",
    "Finally, let's see some topic modelling!\n",
    "\n",
    "Topic modelling is a technique to decompose an entire collection of documents into the topics it is likely to be made of.\n",
    "\n",
    "Specifically, the idea is that the words in each document are not random, but appear because of the topics that each document is about. We know the words in the document (they are the observed variables), then we want to find N topics (and their associated words) that explain as much of all the documents as possible. Here, topic is intended with a more specific meaning that in it's common usage. Rather than being a concept, or a field, we need to interpret a topic as a collection of words and the strength of association between each topic and those words.\n",
    "\n",
    "A commonly used technique to find the topics is called Latent Dirichlet Allocation (LDA). LDA (not to be confused with Linear Discriminant Analysis) finds the topics from a collection of documents, each represented using a count-based Bag of Word model.\n",
    "\n",
    "Training a LDA model means finding the N topics (with N decided by us) that best explain a given collection of documents.\n",
    "\n",
    "Once we find the topics, we can also find the most important words for each topic (typically used to understand what the topic is about) and the most important topic(s) for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f823d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_samples = 6000\n",
    "n_features = 500\n",
    "n_topics = 75\n",
    "n_top_words = 20\n",
    "\n",
    "data_filtered = augmented_tropes.sort_values(by='gender_score').copy()\n",
    "df = pd.concat((data_filtered[:(n_samples//2)],data_filtered[-(n_samples//2):]))\n",
    "\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "tf = tf_vectorizer.fit_transform(df['DescAndExamples'])\n",
    "\n",
    "print(\n",
    "    \"\\n\" * 2,\n",
    "    f\"Fitting LDA models with tf features, n_samples={n_samples} and n_features={n_features}...\"\n",
    ")\n",
    "# create the (untrained) model.\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    max_iter=5,\n",
    "    random_state=0,\n",
    ")\n",
    "# train LDA on our tropes.\n",
    "t0 = time.time()\n",
    "lda.fit(tf)\n",
    "print(f\"done in {(time.time()-t0):.3f} seconds.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afde3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the 20 top words for each topic\n",
    "n_top_words = 20\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "\n",
    "top_words_cols = [f'top_word_{i}' for i in range(20)]\n",
    "top_weights_cols = [f'weight_word_{i}' for i in range(20)]\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    if topic_idx==0:\n",
    "        topic_df = pd.DataFrame(columns = top_words_cols + top_weights_cols) # we'll store all the information here\n",
    "    \n",
    "    # get the indices corresponding to the top words\n",
    "    top_features_ind = topic.argsort()[-n_top_words:]\n",
    "    # link those indices to the top words\n",
    "    top_features = tf_feature_names[top_features_ind]\n",
    "    # get the weight of how important each word is for the topic\n",
    "    weights = topic[top_features_ind]\n",
    "    #topic_df_[top_words_cols]= top_features\n",
    "    #topic_df_[top_weights_cols]= weights\n",
    "    # with each new topic we create a new row for the dataframe which we will then append to the main one\n",
    "    topic_df_ = pd.DataFrame(index= top_words_cols + top_weights_cols, \n",
    "                             data = list(top_features) + list(weights))\n",
    "    #print(topic_df_.T.columns)\n",
    "    topic_df = pd.concat((topic_df, topic_df_.T))\n",
    "\n",
    "topic_df = topic_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989587fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def plot_top_words(model, feature_names, n_top_words, title, topics_ids = range(10)):\n",
    "    # only plots the first 10 topics\n",
    "    assert(len(topics_ids)<=10) #won't work if there are too many topics to plot\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for i, topic_idx in enumerate(topics_ids):\n",
    "        topic = model.components_[topic_idx]\n",
    "        top_features_ind = topic.argsort()[-n_top_words:]\n",
    "        top_features = feature_names[top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[i]\n",
    "        ax.barh(top_features, weights, 0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        ax.set_xlim([0,max(weights)+0.1])\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "        \n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in LDA model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf8897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's use the trained LDA model to project each trope into the topics \n",
    "# based on how much each topic relates to each trope\n",
    "trope_to_topics= lda.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0273784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's show an example: let's plot the distribution of the first document across all topics\n",
    "# The more highly the document depends on a topic, the higher the weight of that document for that topic\n",
    "# (very high weights will appear as \"spikes\" in the plot below)\n",
    "plt.plot(trope_to_topics[0])\n",
    "plt.xlabel('Topic number')\n",
    "_= plt.ylabel('Topic weight for the document')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c6e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so, for each trope we can get the topic that is most highly associated with it (the highest peak in the plot above)\n",
    "# using python's function \"argmax\" (= the index at which the maximum number in a list appears)\n",
    "df['most_probable_topic'] = trope_to_topics.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, for each topic we can compute how many of the tropes associated with it lean \"male\" (gender score <0)\n",
    "# or \"female\" (gender score > 0 )\n",
    "# Finally, we can compute the ratio of the two numbers above\n",
    "topics_to_gender = []\n",
    "topic_df['topic_genderedness'] = 1\n",
    "topic_df['female_leaning'] = pd.NA\n",
    "topic_df['male_leaning'] = pd.NA\n",
    "\n",
    "for topic_nb, g in df.groupby('most_probable_topic'):\n",
    "    nb_male_leaning = (g['gender_score']<0).sum()\n",
    "    nb_female_leaning = (g['gender_score']>0).sum()\n",
    "    if (nb_female_leaning==0) & (nb_male_leaning==0):\n",
    "        r = 1\n",
    "    else:\n",
    "        #we add +1 only to avoid a division by 0 \n",
    "        # (and it's kind of like we are considering 0 a \"rounding error\", it's not very rigorous though).\n",
    "        r = (nb_female_leaning+1)/(nb_male_leaning+1) \n",
    "    topics_to_gender.append(r) \n",
    "    topic_df.loc[topic_nb, 'topic_genderedness'] = r\n",
    "    topic_df.loc[topic_nb, 'female_leaning'] = nb_female_leaning\n",
    "    topic_df.loc[topic_nb, 'male_leaning'] = nb_male_leaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f56b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we say that a topic is \"female leaning\" if a higher proportion of its tropes are \"female leaning\"\n",
    "print(f'Number of \"female leaning\" *topics*: {(topic_df[\"topic_genderedness\"]>1).sum()}')\n",
    "print(f'Number of \"male leaning\" *topics*: {(topic_df[\"topic_genderedness\"]<1).sum()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd7d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df.sort_values(by= ['topic_genderedness','male_leaning'], ascending= [True,False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32119cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take the top 5 \"male\" and \"female\" topics\n",
    "topics_to_plot = topic_df.sort_values(by= ['topic_genderedness','male_leaning'], \n",
    "                                      ascending= [True,False]).index[[0,1,2,3,4,-5,-4,-3,-2,-1]]\n",
    "\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Most gendered topics in LDA model\",\n",
    "              topics_ids= topics_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf7262a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc653559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7eedc23",
   "metadata": {},
   "source": [
    "Disclaimer notice for the code adapted from scikit-learn's documentation:\n",
    "\n",
    "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f1f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
